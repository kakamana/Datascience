{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **School Budgeting with Machine Learning in Python**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can use data science for more than just predicting ad clicks-it can also be used for social good! This course is a case study from DrivenData's machine learning competition. A school district budgeting problem will be explored. Using a model to automatically classify items in a school's budget, schools can compare their spending more quickly. This course begins with a simple, first-pass baseline model. To prepare the budgets for modeling, you'll do some natural language processing. Next, you'll be able to compare your own techniques to those of others. To build the most accurate model, the winner combined a number of expert techniques."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Exploring the raw data**\n",
    "\n",
    "You'll work with school district budgets. Data can be classified according to certain labels, likeÂ Career & Academic Counseling or Librarian.\n",
    "\n",
    "Using some correctly labeled examples, you will develop a model that predicts the probability of each possible label.\n",
    "\n",
    "Because labeled examples will be used to train the model, this is supervised learning.\n",
    "\n",
    "**The algorithm's goal?**\n",
    "\n",
    "The goal is to label budget line items correctly by training a supervised model to predict the probability of each possible label.\n",
    "\n",
    "Predicted probabilities will be used to select a label class.\n",
    "\n",
    "Our classification problem is a multi-class-multi-label problem (quite a mouthful! ), since there are 9 broad categories with many sub-label instances."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "outputs": [],
   "source": [
    "#Exploring the data\n",
    "#Loading the data\n",
    "df = pd.read_csv('TrainingData.csv', index_col=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Loading the data**\n",
    "\n",
    "Now it's time to check out the dataset! You'll use pandas (which has been pre-imported as pd) to load your data into a DataFrame and then do some Exploratory Data Analysis (EDA) of it.\n",
    "\n",
    "Some of the column names correspond to features - descriptions of the budget items - such as the Job_Title_Description column. The values in this column tell us if a budget item is for a teacher, custodian, or other employee.\n",
    "\n",
    "Some columns correspond to the budget item labels you will be trying to predict with your model. For example, the Object_Type column describes whether the budget item is related classroom supplies, salary, travel expenses, etc."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "outputs": [
    {
     "data": {
      "text/plain": "                    Function          Use          Sharing   Reporting  \\\n198                 NO_LABEL     NO_LABEL         NO_LABEL    NO_LABEL   \n209   Student Transportation     NO_LABEL  Shared Services  Non-School   \n750     Teacher Compensation  Instruction  School Reported      School   \n931                 NO_LABEL     NO_LABEL         NO_LABEL    NO_LABEL   \n1524                NO_LABEL     NO_LABEL         NO_LABEL    NO_LABEL   \n\n     Student_Type Position_Type               Object_Type     Pre_K  \\\n198      NO_LABEL      NO_LABEL                  NO_LABEL  NO_LABEL   \n209      NO_LABEL      NO_LABEL    Other Non-Compensation  NO_LABEL   \n750   Unspecified       Teacher  Base Salary/Compensation  Non PreK   \n931      NO_LABEL      NO_LABEL                  NO_LABEL  NO_LABEL   \n1524     NO_LABEL      NO_LABEL                  NO_LABEL  NO_LABEL   \n\n       Operating_Status               Object_Description  ...  \\\n198       Non-Operating                   Supplemental *  ...   \n209   PreK-12 Operating  REPAIR AND MAINTENANCE SERVICES  ...   \n750   PreK-12 Operating     Personal Services - Teachers  ...   \n931       Non-Operating                 General Supplies  ...   \n1524      Non-Operating           Supplies and Materials  ...   \n\n                   Sub_Object_Description Location_Description  FTE  \\\n198   Non-Certificated Salaries And Wages                  NaN  NaN   \n209                                   NaN      ADMIN. SERVICES  NaN   \n750                                   NaN                  NaN  1.0   \n931                      General Supplies                  NaN  NaN   \n1524               Supplies And Materials                  NaN  NaN   \n\n                      Function_Description      Facility_or_Department  \\\n198   Care and Upkeep of Building Services                         NaN   \n209              STUDENT TRANSPORT SERVICE                         NaN   \n750                                    NaN                         NaN   \n931                            Instruction  Instruction And Curriculum   \n1524            Other Community Services *                         NaN   \n\n     Position_Extra     Total  \\\n198             NaN  -8291.86   \n209             NaN    618.29   \n750         TEACHER  49768.82   \n931             NaN     -1.02   \n1524            NaN   2304.43   \n\n                                    Program_Description  \\\n198                                                 NaN   \n209                                PUPIL TRANSPORTATION   \n750                               Instruction - Regular   \n931   \"Title I, Part A Schoolwide Activities Related...   \n1524                                                NaN   \n\n                                       Fund_Description                Text_1  \n198   Title I - Disadvantaged Children/Targeted Assi...    TITLE I CARRYOVER   \n209                                        General Fund                   NaN  \n750                              General Purpose School                   NaN  \n931                              General Operating Fund                   NaN  \n1524  Title I - Disadvantaged Children/Targeted Assi...   TITLE I PI+HOMELESS  \n\n[5 rows x 25 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Function</th>\n      <th>Use</th>\n      <th>Sharing</th>\n      <th>Reporting</th>\n      <th>Student_Type</th>\n      <th>Position_Type</th>\n      <th>Object_Type</th>\n      <th>Pre_K</th>\n      <th>Operating_Status</th>\n      <th>Object_Description</th>\n      <th>...</th>\n      <th>Sub_Object_Description</th>\n      <th>Location_Description</th>\n      <th>FTE</th>\n      <th>Function_Description</th>\n      <th>Facility_or_Department</th>\n      <th>Position_Extra</th>\n      <th>Total</th>\n      <th>Program_Description</th>\n      <th>Fund_Description</th>\n      <th>Text_1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>198</th>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>Non-Operating</td>\n      <td>Supplemental *</td>\n      <td>...</td>\n      <td>Non-Certificated Salaries And Wages</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Care and Upkeep of Building Services</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-8291.86</td>\n      <td>NaN</td>\n      <td>Title I - Disadvantaged Children/Targeted Assi...</td>\n      <td>TITLE I CARRYOVER</td>\n    </tr>\n    <tr>\n      <th>209</th>\n      <td>Student Transportation</td>\n      <td>NO_LABEL</td>\n      <td>Shared Services</td>\n      <td>Non-School</td>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>Other Non-Compensation</td>\n      <td>NO_LABEL</td>\n      <td>PreK-12 Operating</td>\n      <td>REPAIR AND MAINTENANCE SERVICES</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>ADMIN. SERVICES</td>\n      <td>NaN</td>\n      <td>STUDENT TRANSPORT SERVICE</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>618.29</td>\n      <td>PUPIL TRANSPORTATION</td>\n      <td>General Fund</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>750</th>\n      <td>Teacher Compensation</td>\n      <td>Instruction</td>\n      <td>School Reported</td>\n      <td>School</td>\n      <td>Unspecified</td>\n      <td>Teacher</td>\n      <td>Base Salary/Compensation</td>\n      <td>Non PreK</td>\n      <td>PreK-12 Operating</td>\n      <td>Personal Services - Teachers</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>TEACHER</td>\n      <td>49768.82</td>\n      <td>Instruction - Regular</td>\n      <td>General Purpose School</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>931</th>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>Non-Operating</td>\n      <td>General Supplies</td>\n      <td>...</td>\n      <td>General Supplies</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Instruction</td>\n      <td>Instruction And Curriculum</td>\n      <td>NaN</td>\n      <td>-1.02</td>\n      <td>\"Title I, Part A Schoolwide Activities Related...</td>\n      <td>General Operating Fund</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1524</th>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>Non-Operating</td>\n      <td>Supplies and Materials</td>\n      <td>...</td>\n      <td>Supplies And Materials</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Other Community Services *</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2304.43</td>\n      <td>NaN</td>\n      <td>Title I - Disadvantaged Children/Targeted Assi...</td>\n      <td>TITLE I PI+HOMELESS</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã 25 columns</p>\n</div>"
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "outputs": [
    {
     "data": {
      "text/plain": "                        Function          Use                  Sharing  \\\n344986   Substitute Compensation  Instruction          School Reported   \n384803                  NO_LABEL     NO_LABEL                 NO_LABEL   \n224382   Substitute Compensation  Instruction          School Reported   \n305347  Facilities & Maintenance          O&M  Leadership & Management   \n101861      Teacher Compensation  Instruction          School Reported   \n\n         Reporting       Student_Type Position_Type  \\\n344986      School        Unspecified    Substitute   \n384803    NO_LABEL           NO_LABEL      NO_LABEL   \n224382      School  Special Education    Substitute   \n305347  Non-School             Gifted     Custodian   \n101861      School            Poverty       Teacher   \n\n                       Object_Type     Pre_K   Operating_Status  \\\n344986                    Benefits  NO_LABEL  PreK-12 Operating   \n384803                    NO_LABEL  NO_LABEL      Non-Operating   \n224382     Substitute Compensation  NO_LABEL  PreK-12 Operating   \n305347  Other Compensation/Stipend  Non PreK  PreK-12 Operating   \n101861    Base Salary/Compensation  NO_LABEL  PreK-12 Operating   \n\n                                   Object_Description  ...  \\\n344986                              EMPLOYEE BENEFITS  ...   \n384803                              EMPLOYEE BENEFITS  ...   \n224382                 OTHER PERSONAL SERVICES         ...   \n305347  Extra Duty Pay/Overtime For Support Personnel  ...   \n101861                  SALARIES OF REGULAR EMPLOYEES  ...   \n\n                               Sub_Object_Description  Location_Description  \\\n344986                                            NaN                   NaN   \n384803                                            NaN  PERSONNEL-PAID LEAVE   \n224382                                            NaN               School    \n305347  Extra Duty Pay/Overtime For Support Personnel           Unallocated   \n101861                                            NaN                   NaN   \n\n        FTE                   Function_Description Facility_or_Department  \\\n344986  NaN                UNALLOC BUDGETS/SCHOOLS                    NaN   \n384803  NaN                            NON-PROJECT                    NaN   \n224382  0.0         EXCEPTIONAL                                       NaN   \n305347  NaN  Facilities Maintenance And Operations    Gifted And Talented   \n101861  NaN                                TITLE I                    NaN   \n\n                     Position_Extra       Total  \\\n344986   PROFESSIONAL-INSTRUCTIONAL    27.04000   \n384803   PROFESSIONAL-INSTRUCTIONAL         NaN   \n224382                          NaN   200.39000   \n305347  ANY CUS WHO IS NOT A SUPER      5.29000   \n101861   PROFESSIONAL-INSTRUCTIONAL  1575.03504   \n\n                  Program_Description                Fund_Description  \\\n344986  GENERAL HIGH SCHOOL EDUCATION                             NaN   \n384803                 STAFF SERVICES                             NaN   \n224382                            NaN  GENERAL FUND                     \n305347            Gifted And Talented          General Operating Fund   \n101861   GENERAL ELEMENTARY EDUCATION                             NaN   \n\n                               Text_1  \n344986            REGULAR INSTRUCTION  \n384803                        CENTRAL  \n224382                            NaN  \n305347  ADDL REGULAR PAY-NOT SMOOTHED  \n101861            REGULAR INSTRUCTION  \n\n[5 rows x 25 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Function</th>\n      <th>Use</th>\n      <th>Sharing</th>\n      <th>Reporting</th>\n      <th>Student_Type</th>\n      <th>Position_Type</th>\n      <th>Object_Type</th>\n      <th>Pre_K</th>\n      <th>Operating_Status</th>\n      <th>Object_Description</th>\n      <th>...</th>\n      <th>Sub_Object_Description</th>\n      <th>Location_Description</th>\n      <th>FTE</th>\n      <th>Function_Description</th>\n      <th>Facility_or_Department</th>\n      <th>Position_Extra</th>\n      <th>Total</th>\n      <th>Program_Description</th>\n      <th>Fund_Description</th>\n      <th>Text_1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>344986</th>\n      <td>Substitute Compensation</td>\n      <td>Instruction</td>\n      <td>School Reported</td>\n      <td>School</td>\n      <td>Unspecified</td>\n      <td>Substitute</td>\n      <td>Benefits</td>\n      <td>NO_LABEL</td>\n      <td>PreK-12 Operating</td>\n      <td>EMPLOYEE BENEFITS</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>UNALLOC BUDGETS/SCHOOLS</td>\n      <td>NaN</td>\n      <td>PROFESSIONAL-INSTRUCTIONAL</td>\n      <td>27.04000</td>\n      <td>GENERAL HIGH SCHOOL EDUCATION</td>\n      <td>NaN</td>\n      <td>REGULAR INSTRUCTION</td>\n    </tr>\n    <tr>\n      <th>384803</th>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>Non-Operating</td>\n      <td>EMPLOYEE BENEFITS</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>PERSONNEL-PAID LEAVE</td>\n      <td>NaN</td>\n      <td>NON-PROJECT</td>\n      <td>NaN</td>\n      <td>PROFESSIONAL-INSTRUCTIONAL</td>\n      <td>NaN</td>\n      <td>STAFF SERVICES</td>\n      <td>NaN</td>\n      <td>CENTRAL</td>\n    </tr>\n    <tr>\n      <th>224382</th>\n      <td>Substitute Compensation</td>\n      <td>Instruction</td>\n      <td>School Reported</td>\n      <td>School</td>\n      <td>Special Education</td>\n      <td>Substitute</td>\n      <td>Substitute Compensation</td>\n      <td>NO_LABEL</td>\n      <td>PreK-12 Operating</td>\n      <td>OTHER PERSONAL SERVICES</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>School</td>\n      <td>0.0</td>\n      <td>EXCEPTIONAL</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>200.39000</td>\n      <td>NaN</td>\n      <td>GENERAL FUND</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>305347</th>\n      <td>Facilities &amp; Maintenance</td>\n      <td>O&amp;M</td>\n      <td>Leadership &amp; Management</td>\n      <td>Non-School</td>\n      <td>Gifted</td>\n      <td>Custodian</td>\n      <td>Other Compensation/Stipend</td>\n      <td>Non PreK</td>\n      <td>PreK-12 Operating</td>\n      <td>Extra Duty Pay/Overtime For Support Personnel</td>\n      <td>...</td>\n      <td>Extra Duty Pay/Overtime For Support Personnel</td>\n      <td>Unallocated</td>\n      <td>NaN</td>\n      <td>Facilities Maintenance And Operations</td>\n      <td>Gifted And Talented</td>\n      <td>ANY CUS WHO IS NOT A SUPER</td>\n      <td>5.29000</td>\n      <td>Gifted And Talented</td>\n      <td>General Operating Fund</td>\n      <td>ADDL REGULAR PAY-NOT SMOOTHED</td>\n    </tr>\n    <tr>\n      <th>101861</th>\n      <td>Teacher Compensation</td>\n      <td>Instruction</td>\n      <td>School Reported</td>\n      <td>School</td>\n      <td>Poverty</td>\n      <td>Teacher</td>\n      <td>Base Salary/Compensation</td>\n      <td>NO_LABEL</td>\n      <td>PreK-12 Operating</td>\n      <td>SALARIES OF REGULAR EMPLOYEES</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>TITLE I</td>\n      <td>NaN</td>\n      <td>PROFESSIONAL-INSTRUCTIONAL</td>\n      <td>1575.03504</td>\n      <td>GENERAL ELEMENTARY EDUCATION</td>\n      <td>NaN</td>\n      <td>REGULAR INSTRUCTION</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã 25 columns</p>\n</div>"
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1560 entries, 198 to 101861\n",
      "Data columns (total 25 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   Function                1560 non-null   object \n",
      " 1   Use                     1560 non-null   object \n",
      " 2   Sharing                 1560 non-null   object \n",
      " 3   Reporting               1560 non-null   object \n",
      " 4   Student_Type            1560 non-null   object \n",
      " 5   Position_Type           1560 non-null   object \n",
      " 6   Object_Type             1560 non-null   object \n",
      " 7   Pre_K                   1560 non-null   object \n",
      " 8   Operating_Status        1560 non-null   object \n",
      " 9   Object_Description      1461 non-null   object \n",
      " 10  Text_2                  382 non-null    object \n",
      " 11  SubFund_Description     1183 non-null   object \n",
      " 12  Job_Title_Description   1131 non-null   object \n",
      " 13  Text_3                  296 non-null    object \n",
      " 14  Text_4                  193 non-null    object \n",
      " 15  Sub_Object_Description  364 non-null    object \n",
      " 16  Location_Description    874 non-null    object \n",
      " 17  FTE                     449 non-null    float64\n",
      " 18  Function_Description    1340 non-null   object \n",
      " 19  Facility_or_Department  252 non-null    object \n",
      " 20  Position_Extra          1026 non-null   object \n",
      " 21  Total                   1542 non-null   float64\n",
      " 22  Program_Description     1192 non-null   object \n",
      " 23  Fund_Description        819 non-null    object \n",
      " 24  Text_1                  1132 non-null   object \n",
      "dtypes: float64(2), object(23)\n",
      "memory usage: 316.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Data summary**\n",
    "\n",
    "In this exercise, you'll compute summary statistics for the numeric data.\n",
    "\n",
    "You can use df.info() in the IPython Shell to determine which columns are numeric, specifically float64. FTE and Total are two numeric columns.\n",
    "\n",
    "FTE: Full-time equivalent. The percentage of full-time the employee works if the budget item is associated with an employee. The associated employee works full-time for the school. The item is associated with a part-time or contracted employee.\n",
    "\n",
    "Total: Total expenditure. It tells us how much the budget item cost."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "outputs": [
    {
     "data": {
      "text/plain": "              FTE         Total\ncount  449.000000  1.542000e+03\nmean     0.493532  1.446867e+04\nstd      0.452844  7.916752e+04\nmin     -0.002369 -1.044084e+06\n25%      0.004310  1.108111e+02\n50%      0.440000  7.060299e+02\n75%      1.000000  5.347760e+03\nmax      1.047222  1.367500e+06",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>FTE</th>\n      <th>Total</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>449.000000</td>\n      <td>1.542000e+03</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.493532</td>\n      <td>1.446867e+04</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.452844</td>\n      <td>7.916752e+04</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-0.002369</td>\n      <td>-1.044084e+06</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.004310</td>\n      <td>1.108111e+02</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.440000</td>\n      <td>7.060299e+02</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1.000000</td>\n      <td>5.347760e+03</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.047222</td>\n      <td>1.367500e+06</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "outputs": [
    {
     "data": {
      "text/plain": "Text(0, 0.5, 'num employee')"
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df['FTE'].dropna(), bins=10)\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Distribution of %full-time \\n employee works')\n",
    "plt.xlabel('% of full-time')\n",
    "plt.ylabel('num employee')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Looking at the datatypes**\n",
    "\n",
    "\n",
    "*    ML algorithms work on numbers, not strings\n",
    "        Need a numeric representation of these strings\n",
    "*    Strings can be slow compared to numbers\n",
    "    In pandas, category dtype encodes categorical data numerically,\n",
    "        Can speed up code\n",
    "\n",
    "Exploring datatypes in pandas\n",
    "\n",
    "It's always good to know what datatypes you're working with, especially when the inefficient pandas type object may be involved. Towards that end, let's explore what we have."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "outputs": [
    {
     "data": {
      "text/plain": "object     23\nfloat64     2\ndtype: int64"
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes.value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "outputs": [],
   "source": [
    "LABELS = ['Function', 'Use', 'Sharing', 'Reporting', 'Student_Type', 'Position_Type',\n",
    "          'Object_Type', 'Pre_K', 'Operating_Status']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Encode the labels as categorical variables**\n",
    "\n",
    "Your ultimate goal is to predict whether a certain label will be attached to a budget item. Many columns in your data are inefficient object types. Does this include the labels you're predicting? Find out!\n",
    "\n",
    "The dataset has 9 columns of labels. There are many possible values for each of these columns.\n",
    "\n",
    "Every label is encoded as an object datatype. The .astype() method converts labels to category types, which are much more efficient."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function            category\n",
      "Use                 category\n",
      "Sharing             category\n",
      "Reporting           category\n",
      "Student_Type        category\n",
      "Position_Type       category\n",
      "Object_Type         category\n",
      "Pre_K               category\n",
      "Operating_Status    category\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "categorize_label = lambda x: x.astype('category')\n",
    "\n",
    "# Convert df[LABELS] to a category type\n",
    "df[LABELS] = categorize_label(df[LABELS])\n",
    "\n",
    "# Print the converted dtypes\n",
    "print(df[LABELS].dtypes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Counting unique labels**\n",
    "\n",
    "will explore this fact by counting and plotting the number of unique values for each category of label"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate number of unique values for each label: num_unique_labels\n",
    "num_unique_labels = df[LABELS].apply(pd.Series.nunique)\n",
    "\n",
    "# Plot number of unique values for each label\n",
    "num_unique_labels.plot(kind='bar')\n",
    "\n",
    "# Label the axes\n",
    "plt.xlabel('Labels')\n",
    "plt.ylabel('Number of unique values')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **How do we measure success?**\n",
    "\n",
    "    Accuracy can be misleading when classes are imbalanced\n",
    "        Legitmate email: 99%, Spam: 1%\n",
    "        Model that never predicts spam will be 99% accurate!\n",
    "    Metric used in this problem: log loss\n",
    "        Loss function\n",
    "        Measure of error\n",
    "        Want to minimize the error (unlike accuracy)\n",
    "  Log loss binary classification\n",
    "\n",
    "  logloss = -$\\frac{1}{N} \\Sigma_{i=1}^N (y{i} \\log(p{i})) + (1 - y{i}) \\log(1-p{i})$\n",
    "\n",
    "Actual value: y:1=yes,0=noy:1=yes,0=no <br>\n",
    "Prediction (probability that the value is 1): pp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Computing log loss with NumPy**\n",
    "\n",
    "To see how the log loss metric handles the trade-off between accuracy and confidence, we will use some sample data generated with NumPy and compute the log loss using the provided function compute_log_loss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "outputs": [],
   "source": [
    "def compute_log_loss(predicted, actual, eps=1e-14):\n",
    "    \"\"\"Compute the logarithmic loss between predicted and\n",
    "       actual when these are 1D arrays\n",
    "\n",
    "       :param predicted: The predicted probabilties as floats between 0-1\n",
    "       :param actual: The actual binary labels. Either 0 or 1\n",
    "       :param eps (optional): log(0) is inf, so we need to offset our\n",
    "                               predicted values slightly by eps from 0 or 1.\n",
    "    \"\"\"\n",
    "    predicted = np.clip(predicted, eps, 1-eps)\n",
    "    loss = -1 * np.mean(actual * np.log(predicted) + (1 - actual) * np.log(1 - predicted))\n",
    "    return loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "outputs": [],
   "source": [
    "correct_confident = np.array([0.95, 0.95, 0.95, 0.95, 0.95, 0.05, 0.05, 0.05, 0.05, 0.05])\n",
    "correct_not_confident = np.array([0.65, 0.65, 0.65, 0.65, 0.65, 0.35, 0.35, 0.35, 0.35, 0.35])\n",
    "wrong_not_confident = np.array([0.35, 0.35, 0.35, 0.35, 0.35, 0.65, 0.65, 0.65, 0.65, 0.65])\n",
    "wrong_confident = np.array([0.05, 0.05, 0.05, 0.05, 0.05, 0.95, 0.95, 0.95, 0.95, 0.95])\n",
    "actual_labels = np.array([1., 1., 1., 1., 1., 0., 0., 0., 0., 0.])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss, correct and confident: 0.05129329438755058\n",
      "Log loss, correct and not confident: 0.4307829160924542\n",
      "Log loss, wrong and not confident: 1.049822124498678\n",
      "Log loss, wrong and confident: 2.9957322735539904\n",
      "Log loss, actual labels: 9.99200722162646e-15\n"
     ]
    }
   ],
   "source": [
    "correct_confident_loss = compute_log_loss(correct_confident, actual_labels)\n",
    "print(\"Log loss, correct and confident: {}\".format(correct_confident_loss))\n",
    "\n",
    "# Compute log loss for 2nd case\n",
    "correct_not_confident_loss = compute_log_loss(correct_not_confident, actual_labels)\n",
    "print(\"Log loss, correct and not confident: {}\".format(correct_not_confident_loss))\n",
    "\n",
    "# Compute and print log loss for 3rd case\n",
    "wrong_not_confident_loss = compute_log_loss(wrong_not_confident, actual_labels)\n",
    "print(\"Log loss, wrong and not confident: {}\".format(wrong_not_confident_loss))\n",
    "\n",
    "# Compute and print log loss for 4th case\n",
    "wrong_confident_loss = compute_log_loss(wrong_confident, actual_labels)\n",
    "print(\"Log loss, wrong and confident: {}\".format(wrong_confident_loss))\n",
    "\n",
    "# Compute and print log loss for actual labels\n",
    "actual_labels_loss = compute_log_loss(actual_labels, actual_labels)\n",
    "print(\"Log loss, actual labels: {}\".format(actual_labels_loss))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. **Creating a simple first model**\n",
    "\n",
    "It's time to build the model\n",
    "\n",
    "*   Starting with a very simple model is always a good idea\n",
    "*   Shows how challenging the problem is\n",
    "*   Complex models can go wrong in many ways\n",
    "*   Basic methods: how much signal can we get?\n",
    "*   Numeric data only for training\n",
    "  *  Predictions from raw data\n",
    "*   Multiclass logistic regression\n",
    "  * Classify each label separately and predict\n",
    "* Format predictions and save to csv\n",
    "* Compute log loss score\n",
    "* Multi-class dataset split\n",
    "  * Recall: Train-test split\n",
    "    * Will not work here\n",
    "    * Test set labels may not match training set labels\n",
    "  * Solution: StratifiedShyffleSplit\n",
    "    * Only works with a single target variable"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Setting up a train-test split in scikit-learn**\n",
    "\n",
    "Alright, you've been patient and awesome. It's finally time to start training models!\n",
    "\n",
    "The first step is to split the data into a training set and a test set. Some labels don't occur very often, but we want to make sure that they appear in both the training and the test sets. We provide a function that will make sure at least min_count examples of each label appear in each split: multilabel_train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "outputs": [],
   "source": [
    "from warnings import warn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def multilabel_sample(y, size=1000, min_count=5, seed=None):\n",
    "    \"\"\" Takes a matrix of binary labels `y` and returns\n",
    "        the indices for a sample of size `size` if\n",
    "        `size` > 1 or `size` * len(y) if size =< 1.\n",
    "        The sample is guaranteed to have > `min_count` of\n",
    "        each label.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if (np.unique(y).astype(int) != np.array([0, 1])).any():\n",
    "            raise ValueError()\n",
    "    except (TypeError, ValueError):\n",
    "        raise ValueError('multilabel_sample only works with binary indicator matrices')\n",
    "\n",
    "    if (y.sum(axis=0) < min_count).any():\n",
    "        raise ValueError('Some classes do not have enough examples. Change min_count if necessary.')\n",
    "\n",
    "    if size <= 1:\n",
    "        size = np.floor(y.shape[0] * size)\n",
    "\n",
    "    if y.shape[1] * min_count > size:\n",
    "        msg = \"Size less than number of columns * min_count, returning {} items instead of {}.\"\n",
    "        warn(msg.format(y.shape[1] * min_count, size))\n",
    "        size = y.shape[1] * min_count\n",
    "\n",
    "    rng = np.random.RandomState(seed if seed is not None else np.random.randint(1))\n",
    "\n",
    "    if isinstance(y, pd.DataFrame):\n",
    "        choices = y.index\n",
    "        y = y.values\n",
    "    else:\n",
    "        choices = np.arange(y.shape[0])\n",
    "\n",
    "    sample_idxs = np.array([], dtype=choices.dtype)\n",
    "\n",
    "    # first, guarantee > min_count of each label\n",
    "    for j in range(y.shape[1]):\n",
    "        label_choices = choices[y[:, j] == 1]\n",
    "        label_idxs_sampled = rng.choice(label_choices, size=min_count, replace=False)\n",
    "        sample_idxs = np.concatenate([label_idxs_sampled, sample_idxs])\n",
    "\n",
    "    sample_idxs = np.unique(sample_idxs)\n",
    "\n",
    "    # now that we have at least min_count of each, we can just random sample\n",
    "    sample_count = int(size - sample_idxs.shape[0])\n",
    "\n",
    "    # get sample_count indices from remaining choices\n",
    "    remaining_choices = np.setdiff1d(choices, sample_idxs)\n",
    "    remaining_sampled = rng.choice(remaining_choices,\n",
    "                                   size=sample_count,\n",
    "                                   replace=False)\n",
    "\n",
    "    return np.concatenate([sample_idxs, remaining_sampled])\n",
    "\n",
    "\n",
    "def multilabel_sample_dataframe(df, labels, size, min_count=5, seed=None):\n",
    "    \"\"\" Takes a dataframe `df` and returns a sample of size `size` where all\n",
    "        classes in the binary matrix `labels` are represented at\n",
    "        least `min_count` times.\n",
    "    \"\"\"\n",
    "    idxs = multilabel_sample(labels, size=size, min_count=min_count, seed=seed)\n",
    "    return df.loc[idxs]\n",
    "\n",
    "\n",
    "def multilabel_train_test_split(X, Y, size, min_count=5, seed=None):\n",
    "    \"\"\" Takes a features matrix `X` and a label matrix `Y` and\n",
    "        returns (X_train, X_test, Y_train, Y_test) where all\n",
    "        classes in Y are represented at least `min_count` times.\n",
    "    \"\"\"\n",
    "    index = Y.index if isinstance(Y, pd.DataFrame) else np.arange(Y.shape[0])\n",
    "\n",
    "    test_set_idxs = multilabel_sample(Y, size=size, min_count=min_count, seed=seed)\n",
    "    train_set_idxs = np.setdiff1d(index, test_set_idxs)\n",
    "\n",
    "    test_set_mask = index.isin(test_set_idxs)\n",
    "    train_set_mask = ~test_set_mask\n",
    "\n",
    "    return (X[train_set_mask], X[test_set_mask], Y[train_set_mask], Y[test_set_mask])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You'll start with a simple model that uses just the numeric columns of your DataFrame when calling multilabel_train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "outputs": [],
   "source": [
    "NUMERIC_COLUMNS = ['FTE', 'Total']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "outputs": [
    {
     "data": {
      "text/plain": "     FTE     Total\n198  NaN  -8291.86\n209  NaN    618.29\n750  1.0  49768.82",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>FTE</th>\n      <th>Total</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>198</th>\n      <td>NaN</td>\n      <td>-8291.86</td>\n    </tr>\n    <tr>\n      <th>209</th>\n      <td>NaN</td>\n      <td>618.29</td>\n    </tr>\n    <tr>\n      <th>750</th>\n      <td>1.0</td>\n      <td>49768.82</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[NUMERIC_COLUMNS].head(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "outputs": [
    {
     "data": {
      "text/plain": "(1560, 2)"
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[NUMERIC_COLUMNS].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['Function_Aides Compensation', 'Function_Career & Academic Counseling',\n       'Function_Communications', 'Function_Curriculum Development',\n       'Function_Data Processing & Information Services',\n       'Function_Development & Fundraising', 'Function_Enrichment',\n       'Function_Extended Time & Tutoring',\n       'Function_Facilities & Maintenance', 'Function_Facilities Planning',\n       ...\n       'Object_Type_Rent/Utilities', 'Object_Type_Substitute Compensation',\n       'Object_Type_Supplies/Materials', 'Object_Type_Travel & Conferences',\n       'Pre_K_NO_LABEL', 'Pre_K_Non PreK', 'Pre_K_PreK',\n       'Operating_Status_Non-Operating',\n       'Operating_Status_Operating, Not PreK-12',\n       'Operating_Status_PreK-12 Operating'],\n      dtype='object', length=104)"
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dummies.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "outputs": [
    {
     "data": {
      "text/plain": "(1560, 104)"
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dummies.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1040 entries, 198 to 101861\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   FTE     1040 non-null   float64\n",
      " 1   Total   1040 non-null   float64\n",
      "dtypes: float64(2)\n",
      "memory usage: 24.4 KB\n",
      "None\n",
      "\n",
      "X_test info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 520 entries, 209 to 448628\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   FTE     520 non-null    float64\n",
      " 1   Total   520 non-null    float64\n",
      "dtypes: float64(2)\n",
      "memory usage: 12.2 KB\n",
      "None\n",
      "\n",
      "y_train info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1040 entries, 198 to 101861\n",
      "Columns: 104 entries, Function_Aides Compensation to Operating_Status_PreK-12 Operating\n",
      "dtypes: uint8(104)\n",
      "memory usage: 113.8 KB\n",
      "None\n",
      "\n",
      "y_test info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 520 entries, 209 to 448628\n",
      "Columns: 104 entries, Function_Aides Compensation to Operating_Status_PreK-12 Operating\n",
      "dtypes: uint8(104)\n",
      "memory usage: 56.9 KB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_31324\\430707777.py:27: UserWarning: Size less than number of columns * min_count, returning 520 items instead of 312.0.\n",
      "  warn(msg.format(y.shape[1] * min_count, size))\n"
     ]
    }
   ],
   "source": [
    "# Create the new DataFrame: numeric_data_only\n",
    "numeric_data_only = df[NUMERIC_COLUMNS].fillna(-1000)\n",
    "\n",
    "# Get labels and convert to dummy variables: label_dummies\n",
    "label_dummies = pd.get_dummies(df[LABELS])\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = multilabel_train_test_split(numeric_data_only,label_dummies,size=0.2,seed=123)\n",
    "\n",
    "# Print the info\n",
    "print(\"X_train info:\")\n",
    "print(X_train.info())\n",
    "print(\"\\nX_test info:\")\n",
    "print(X_test.info())\n",
    "print(\"\\ny_train info:\")\n",
    "print(y_train.info())\n",
    "print(\"\\ny_test info:\")\n",
    "print(y_test.info())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Training a model**\n",
    "\n",
    "With split data in hand, you're only a few lines away from training a model.\n",
    "\n",
    "In this exercise, you will import the logistic regression and one versus rest classifiers in order to fit a multi-class logistic regression model to the NUMERIC_COLUMNS of your feature data.\n",
    "\n",
    "Then you'll test and print the accuracy with the .score() method to see the results of training.\n",
    "\n",
    "Before you train! Remember, we're ultimately going to be using logloss to score our model, so don't worry too much about the accuracy here. Keep in mind that you're throwing away all of the text data in the dataset - that's by far most of the data! So don't get your hopes up for a killer performance just yet. We're just interested in getting things up and running at the moment."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dghr201\\AppData\\Local\\Temp\\ipykernel_31324\\430707777.py:27: UserWarning: Size less than number of columns * min_count, returning 520 items instead of 312.0.\n",
      "  warn(msg.format(y.shape[1] * min_count, size))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Import classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Create the DataFrame: numeric_data_only\n",
    "numeric_data_only = df[NUMERIC_COLUMNS].fillna(-1000)\n",
    "\n",
    "# Get labels and convert to dummy variables: label_dummies\n",
    "label_dummies = pd.get_dummies(df[LABELS])\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = multilabel_train_test_split(numeric_data_only,\n",
    "                                                               label_dummies,\n",
    "                                                               size=0.2,\n",
    "                                                               seed=123)\n",
    "\n",
    "# Instantiate the classifier: clf\n",
    "clf = OneVsRestClassifier(LogisticRegression())\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy: {}\".format(clf.score(X_test, y_test)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The good news is that your workflow didn't cause any errors. The bad news is that your model scored the lowest possible accuracy: 0.0! But hey, you just threw away ALL of the text data in the budget. Later, you won't. Before you add the text data, let's see how the model does when scored by log loss."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Making predictions**\n",
    "Use your model to predict values on holdout data\n",
    "\n",
    "You're ready to make some predictions! Remember, the train-test-split you've carried out so far is for model development. The original competition provides an additional test set, for which you'll never actually see the correct labels. This is called the \"holdout data.\"\n",
    "\n",
    "The point of the holdout data is to provide a fair test for machine learning competitions. If the labels aren't known by anyone but DataCamp, DrivenData, or whoever is hosting the competition, you can be sure that no one submits a mere copy of labels to artificially pump up the performance on their model.\n",
    "\n",
    "Remember that the original goal is to predict the probability of each label. In this exercise you'll do just that by using the .predict_proba() method on your trained model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "outputs": [],
   "source": [
    "# Instantiate the classifier: clf\n",
    "clf = OneVsRestClassifier(LogisticRegression())\n",
    "# Fit it to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "# Load the holdout data: holdout\n",
    "holdout = pd.read_csv('HoldoutData.csv', index_col=0)\n",
    "holdout = holdout[NUMERIC_COLUMNS].fillna(-1000)\n",
    "# Generate predictions: predictions\n",
    "predictions = clf.predict_proba(holdout)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Writing out your results to a csv for submission**\n",
    "\n",
    "At last, you're ready to submit some predictions for scoring. In this exercise, you'll write your predictions to a .csv using the .to_csv() method on a pandas DataFrame. Then you'll evaluate your performance according to the LogLoss metric discussed earlier!\n",
    "\n",
    "You'll need to make sure your submission obeys the correct format.\n",
    "\n",
    "To do this, you'll use your predictions values to create a new DataFrame, prediction_df.\n",
    "\n",
    "Interpreting LogLoss & Beating the Benchmark:\n",
    "\n",
    "When interpreting your log loss score, keep in mind that the score will change based on the number of samples tested. To get a sense of how this very basic model performs, compare your score to the DrivenData benchmark model performance: 2.0455, which merely submitted uniform probabilities for each class.\n",
    "\n",
    "Remember, the lower the log loss the better. Is your model's log loss lower than 2.0455?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "outputs": [],
   "source": [
    "BOX_PLOTS_COLUMN_INDICES = [range(0, 37),\n",
    " range(37, 48),\n",
    " range(48, 51),\n",
    " range(51, 76),\n",
    " range(76, 79),\n",
    " range(79, 82),\n",
    " range(82, 87),\n",
    " range(87, 96),\n",
    " range(96, 104)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "outputs": [],
   "source": [
    "def _multi_multi_log_loss(predicted,\n",
    "                          actual,\n",
    "                          class_column_indices=BOX_PLOTS_COLUMN_INDICES,\n",
    "                          eps=1e-15):\n",
    "    \"\"\" Multi class version of Logarithmic Loss metric as implemented on\n",
    "    DrivenData.org\n",
    "    \"\"\"\n",
    "    class_scores = np.ones(len(class_column_indices), dtype=np.float64)\n",
    "\n",
    "    # calculate log loss for each set of columns that belong to a class:\n",
    "    for k, this_class_indices in enumerate(class_column_indices):\n",
    "        # get just the columns for this class\n",
    "        preds_k = predicted[:, this_class_indices].astype(np.float64)\n",
    "\n",
    "        # normalize so probabilities sum to one (unless sum is zero, then we clip)\n",
    "        preds_k /= np.clip(preds_k.sum(axis=1).reshape(-1, 1), eps, np.inf)\n",
    "\n",
    "        actual_k = actual[:, this_class_indices]\n",
    "\n",
    "        # shrink predictions so\n",
    "        y_hats = np.clip(preds_k, eps, 1 - eps)\n",
    "        sum_logs = np.sum(actual_k * np.log(y_hats))\n",
    "        class_scores[k] = (-1.0 / actual.shape[0]) * sum_logs\n",
    "\n",
    "    return np.average(class_scores)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "outputs": [],
   "source": [
    "def score_submission(pred_path='./', holdout_path='https://s3.amazonaws.com/assets.datacamp.com/production/course_2826/datasets/TestSetLabelsSample.csv'):\n",
    "    # this happens on the backend to get the score\n",
    "    holdout_labels = pd.get_dummies(\n",
    "        pd.read_csv(holdout_path, index_col=0)\n",
    "        .apply(lambda x: x.astype('category'), axis=0)\n",
    "    )\n",
    "\n",
    "    preds = pd.read_csv(pred_path, index_col=0)\n",
    "\n",
    "    # make sure that format is correct\n",
    "    assert (preds.columns == holdout_labels.columns).all()\n",
    "    assert (preds.index == holdout_labels.index).all()\n",
    "\n",
    "    return _multi_multi_log_loss(preds.values, holdout_labels.values)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your model, trained with numeric data only, yields logloss score: 1.992234329191831\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions: predictions\n",
    "predictions = clf.predict_proba(holdout[NUMERIC_COLUMNS].fillna(-1000))\n",
    "\n",
    "# Format predictions in DataFrame: prediction_df\n",
    "prediction_df = pd.DataFrame(columns=pd.get_dummies(df[LABELS]).columns,\n",
    "                             index=holdout.index,\n",
    "                             data=predictions)\n",
    "\n",
    "\n",
    "# Save prediction_df to csv\n",
    "prediction_df.to_csv('predictions.csv')\n",
    "\n",
    "# Submit the predictions for scoring: score\n",
    "score = score_submission(pred_path='predictions.csv')\n",
    "\n",
    "# Print score\n",
    "print('Your model, trained with numeric data only, yields logloss score: {}'.format(score))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A very brief introduction to NLP\n",
    "\n",
    "    A very brief introduction to NLP\n",
    "        Data fpr NLP:\n",
    "            Text, documents, speech,...\n",
    "        Tokenization\n",
    "            Spliting a string into segments\n",
    "            Store segments as list\n",
    "        Example: \"Natural Langauge Processing\" -> [\"Natural\", \"Language\", \"Processing\"]\n",
    "    Bag of words representation\n",
    "        Count the number of times a particular token appears\n",
    "        \"Bag of words\"\n",
    "            Count the number of times a word was pulled out of the bag\n",
    "        This approach discards information about word order\n",
    "            \"Red, not blue\" is the same as \"blue, not red\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Representing text numerically\n",
    "\n",
    "    Representing text numerically\n",
    "        Bag-of-words\n",
    "            Simple way to represent text in machine learning\n",
    "            Discards information about grammar and word order\n",
    "            Computes frequency of occurance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "n_grams token\n",
    "\n",
    "    one_grams = [âpetroâ, âvendâ, âfuelâ, âandâ, âfluidsâ]\n",
    "    two _grams = [âpetro vendâ, âvend fuelâ, âfuel andâ, âand fluidsâ]\n",
    "    three _grams = [âpetro vend fuelâ, âvend fuel andâ, â fuel and fluidsâ]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating a bag-of-words in scikit-learn\n",
    "\n",
    "In this exercise, you'll study the effects of tokenizing in different ways by comparing the bag-of-words representations resulting from different token patterns.\n",
    "\n",
    "You will focus on one feature only, the Position_Extra column, which describes any additional information not captured by the Position_Type label.\n",
    "\n",
    "For example, in the Shell you can check out the budget item in row 8960 of the data using df.loc[8960]. Looking at the output reveals that this Object_Description is overtime pay. For who? The Position Type is merely \"other\", but the Position Extra elaborates: \"BUS DRIVER\". Explore the column further to see more instances. It has a lot of NaN values.\n",
    "\n",
    "Your task is to turn the raw text in this column into a bag-of-words representation by creating tokens that contain only alphanumeric characters.\n",
    "\n",
    "For comparison purposes, the first 15 tokens of vec_basic, which splits df.Position_Extra into tokens when it encounters only whitespace characters, have been printed along with the length of the representation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 123 tokens in Position_Extra if we split on non-alpha numeric\n",
      "['1st', '2nd', '3rd', 'a', 'ab', 'additional', 'adm', 'administrative', 'and', 'any', 'art', 'assessment', 'assistant', 'asst', 'athletic']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "\n",
    "# Fill missing values in df.Position_Extra\n",
    "df.Position_Extra.fillna('', inplace=True)\n",
    "\n",
    "# Instantiate the CountVectorizer: vec_alphanumeric\n",
    "vec_alphanumeric = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n",
    "\n",
    "# Fit to the data\n",
    "vec_alphanumeric.fit(df.Position_Extra)\n",
    "\n",
    "# Print the number of tokens and first 15 tokens\n",
    "msg = \"There are {} tokens in Position_Extra if we split on non-alpha numeric\"\n",
    "print(msg.format(len(vec_alphanumeric.get_feature_names())))\n",
    "print(vec_alphanumeric.get_feature_names()[:15])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Combining text columns for tokenization\n",
    "\n",
    "In order to get a bag-of-words representation for all of the text data in our DataFrame, you must first convert the text data in each row of the DataFrame into a single string.\n",
    "\n",
    "In the previous exercise, this wasnât necessary because you only looked at one column of data, so each row was already just a single string.Â CountVectorizerÂ expects each row to just be a single string, so in order to use all of the text columns, youâll need a method to turn a list of strings into a single string.\n",
    "\n",
    "In this exercise, youâll complete the function definitionÂ combine_text_columns(). When completed, this function will convert all training text data in your DataFrame to a single string per row that can be passed to the vectorizer object and made into a bag-of-words using theÂ .fit_transform()Â method.\n",
    "\n",
    "Note that the function usesÂ NUMERIC_COLUMNSÂ andÂ LABELSÂ to determine which columns to drop. These lists have been loaded into the workspace."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "outputs": [],
   "source": [
    "# Define combine_text_columns()\n",
    "def combine_text_columns(data_frame, to_drop=NUMERIC_COLUMNS + LABELS):\n",
    "    \"\"\" converts all text in each row of data_frame to single vector \"\"\"\n",
    "\n",
    "    # Drop non-text columns that are in the df\n",
    "    to_drop = set(to_drop) & set(data_frame.columns.tolist())\n",
    "    text_data = data_frame.drop(to_drop,axis=1)\n",
    "\n",
    "    # Replace nans with blanks\n",
    "    text_data.fillna('',inplace=True)\n",
    "\n",
    "    # Join all text items in a row that have a space in between\n",
    "    return text_data.apply(lambda x: \" \".join(x), axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1404 tokens in the dataset\n",
      "There are 1117 alpha-numeric tokens in the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dghr201\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import the CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create the basic token pattern\n",
    "TOKENS_BASIC = '\\\\S+(?=\\\\s+)'\n",
    "\n",
    "# Create the alphanumeric token pattern\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "\n",
    "# Instantiate basic CountVectorizer: vec_basic\n",
    "vec_basic = CountVectorizer(token_pattern=TOKENS_BASIC)\n",
    "\n",
    "# Instantiate alphanumeric CountVectorizer: vec_alphanumeric\n",
    "vec_alphanumeric = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n",
    "\n",
    "# Create the text vector\n",
    "text_vector = combine_text_columns(df)\n",
    "\n",
    "# Fit and transform vec_basic\n",
    "vec_basic.fit_transform(text_vector)\n",
    "\n",
    "# Print number of tokens of vec_basic\n",
    "print(\"There are {} tokens in the dataset\".format(len(vec_basic.get_feature_names())))\n",
    "\n",
    "# Fit and transform vec_alphanumeric\n",
    "vec_alphanumeric.fit_transform(text_vector)\n",
    "\n",
    "# Print number of tokens of vec_alphanumeric\n",
    "print(\"There are {} alpha-numeric tokens in the dataset\".format(len(vec_alphanumeric.get_feature_names())))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Improving your model\n",
    "\n",
    "## Pipelines, feature & text preprocessing\n",
    "\n",
    "### Why pipelines?\n",
    "\n",
    "Check out the documention [here](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline). An example [follows](https://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html).\n",
    "Pipelines combine several machine learning steps (like scaling data and tuning parameters).\n",
    "\n",
    "### Pipeline: why use it?\n",
    "\n",
    "* Reading your code will be easier\n",
    "* Parameters can be modified easily\n",
    "\n",
    "### Pipeline workflow\n",
    "\n",
    "* Raw data to trained model in a repeatable way\n",
    "* Sequential list of steps in pipeline\n",
    "    * A step's output is input to the next\n",
    "\n",
    "* A tuple has two elements per step\n",
    "\n",
    "    * Name: String\n",
    "\n",
    "    * Transform: obj implementing .fit() and .transform()\n",
    "\n",
    "* Flexible: a step can itself be another pipeline!\n",
    "\n",
    "\n",
    "### Instantiate pipeline\n",
    "\n",
    "Here, you'll review pipelines and train a classifier on synthetic (sample) data of multiple types before applying the same techniques to the main dataset.\n",
    "\n",
    "Your task is to create a pipeline that trains on the numeric column of the sample data.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](https://github.com/kakamana/Datascience/blob/main/Datacamp/Case%20Study:%20School%20Budgeting%20with%20Machine%20Learning%20in%20Python/InstantiatePipelineImg.png)\n",
    "\n",
    "![](InstantiatePipelineImg.png)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "outputs": [],
   "source": [
    "sample_df = pd.read_csv('sample_data.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on sample data - numeric, no nans:  0.62\n"
     ]
    }
   ],
   "source": [
    "# Import Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Import other necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Split and select numeric data only, no nans\n",
    "X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric']],\n",
    "                                                    pd.get_dummies(sample_df['label']),\n",
    "                                                    random_state=22)\n",
    "\n",
    "# Instantiate Pipeline object: pl\n",
    "pl = Pipeline([\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on sample data - numeric, no nans: \", accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "outputs": [
    {
     "data": {
      "text/plain": "     Unnamed: 0    numeric     text  with_missing label\n0             0 -10.856306      NaN      4.433240     b\n1             1   9.973454      foo      4.310229     b\n2             2   2.829785  foo bar      2.469828     a\n3             3 -15.062947      NaN      2.852981     b\n4             4  -5.786003  foo bar      1.826475     a\n..          ...        ...      ...           ...   ...\n995         995   6.347631      foo      3.140256     b\n996         996  10.699186      bar           NaN     a\n997         997  -9.093270      NaN      4.132525     b\n998         998   4.702637  foo bar           NaN     a\n999         999 -11.114304  foo bar      1.963396     b\n\n[1000 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>numeric</th>\n      <th>text</th>\n      <th>with_missing</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>-10.856306</td>\n      <td>NaN</td>\n      <td>4.433240</td>\n      <td>b</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>9.973454</td>\n      <td>foo</td>\n      <td>4.310229</td>\n      <td>b</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>2.829785</td>\n      <td>foo bar</td>\n      <td>2.469828</td>\n      <td>a</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>-15.062947</td>\n      <td>NaN</td>\n      <td>2.852981</td>\n      <td>b</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>-5.786003</td>\n      <td>foo bar</td>\n      <td>1.826475</td>\n      <td>a</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>995</th>\n      <td>995</td>\n      <td>6.347631</td>\n      <td>foo</td>\n      <td>3.140256</td>\n      <td>b</td>\n    </tr>\n    <tr>\n      <th>996</th>\n      <td>996</td>\n      <td>10.699186</td>\n      <td>bar</td>\n      <td>NaN</td>\n      <td>a</td>\n    </tr>\n    <tr>\n      <th>997</th>\n      <td>997</td>\n      <td>-9.093270</td>\n      <td>NaN</td>\n      <td>4.132525</td>\n      <td>b</td>\n    </tr>\n    <tr>\n      <th>998</th>\n      <td>998</td>\n      <td>4.702637</td>\n      <td>foo bar</td>\n      <td>NaN</td>\n      <td>a</td>\n    </tr>\n    <tr>\n      <th>999</th>\n      <td>999</td>\n      <td>-11.114304</td>\n      <td>foo bar</td>\n      <td>1.963396</td>\n      <td>b</td>\n    </tr>\n  </tbody>\n</table>\n<p>1000 rows Ã 5 columns</p>\n</div>"
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing numeric features\n",
    "\n",
    "To fill in missing values in your sample data, you'll use the Imputer() [imputation transformer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html) from scikit-learn.\n",
    "\n",
    "By default, the imputer transformer replaces NaNs with the column mean. You won't need to pass anything extra to the imputer since that's a good imputation strategy.\n",
    "\n",
    "Once the transformer has been imported, edit the steps list by inserting a (name, transform) tuple. Steps are processed sequentially, so place the new tuple encoding your preprocessing step correctly"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on sample data - all numeric, incl nans:  0.636\n"
     ]
    }
   ],
   "source": [
    "# Import the Imputer object\n",
    "#from sklearn.preprocessing import Imputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create training and test sets using only numeric data\n",
    "X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric', 'with_missing']],\n",
    "                                                    pd.get_dummies(sample_df['label']),\n",
    "                                                    random_state=456)\n",
    "\n",
    "imputer=SimpleImputer(missing_values=np.nan,strategy='mean')\n",
    "# Insantiate Pipeline object: pl\n",
    "pl = Pipeline([\n",
    "        ('imp', imputer),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pl.fit(X_train,y_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test,y_test)\n",
    "print(\"\\nAccuracy on sample data - all numeric, incl nans: \", accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now you know how to use preprocessing in pipelines with numeric data, and it looks like the accuracy has improved because of it! Text data preprocessing is next!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Text features and feature unions\n",
    "\n",
    "* Preprocessing multiple dtypes\n",
    "    * Want to use all avaialbe features in one pipeline\n",
    "    * Problem\n",
    "        * Pipeline stpes for numeric & text processing can't follow each other\n",
    "        * E.g. output of **CounterVectorizer** can't be input to **Imputer**\n",
    "    * Solution\n",
    "        * **FunctionTransformer() & FeatureUnion()\n",
    "* FunctionTransformer\n",
    "    * Turns a python function into an object that a scikit-learn pipeline can understand\n",
    "    * Need to write two functions for pipeline preprocessing\n",
    "        * Take entire DataFrame, return numeric columns\n",
    "        * Take entire DataFrame, return text columns\n",
    "    * Can then preprocess numeric and text data in sperate pipeline\n",
    "\n",
    "### Preprocessing text features\n",
    "\n",
    "Using the text column from the sample data, you'll perform a similar preprocessing pipeline step.\n",
    "To preprocess the text, you will use CountVectorizer() to generate a bag-of-words representation. Add a tuple (step, transform) to the pipeline steps list with the default arguments.\n",
    "When splitting your training and test sets, only select the text column"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on sample data - just text data:  0.808\n"
     ]
    }
   ],
   "source": [
    "sample_df['text'] = sample_df['text'].fillna(\"\")\n",
    "\n",
    "# Import the CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Split out only the text data\n",
    "X_train, X_test, y_train, y_test = train_test_split(sample_df['text'],\n",
    "                                                    pd.get_dummies(sample_df['label']),\n",
    "                                                    random_state=456)\n",
    "\n",
    "# Instantiate Pipeline object: pl\n",
    "pl = Pipeline([\n",
    "        ('vec', CountVectorizer()),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])\n",
    "\n",
    "# Fit to the training data\n",
    "pl.fit(X_train,y_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test,y_test)\n",
    "print(\"\\nAccuracy on sample data - just text data: \", accuracy)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### FunctionTransformer: multiple processing types\n",
    "\n",
    "You'll learn new topics in the next two exercises that will improve your pipeline.\n",
    "\n",
    "Fit and transform methods must be implemented by all steps in the pipeline. Any Python function you pass to the FunctionTransformer is transformed into this object. It will be used to select subsets of data in a pipeline-friendly way.\n",
    "\n",
    "You work with numeric data that needs imputation, and text data that needs conversion. You will create functions that separate text from numeric variables and learn how .fit() and .transform() work."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Data\n",
      "0           \n",
      "1        foo\n",
      "2    foo bar\n",
      "3           \n",
      "4    foo bar\n",
      "Name: text, dtype: object\n",
      "\n",
      "Numeric Data\n",
      "     numeric  with_missing\n",
      "0 -10.856306      4.433240\n",
      "1   9.973454      4.310229\n",
      "2   2.829785      2.469828\n",
      "3 -15.062947      2.852981\n",
      "4  -5.786003      1.826475\n"
     ]
    }
   ],
   "source": [
    "# Import FunctionTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Obtain the text data: get_text_data\n",
    "get_text_data = FunctionTransformer(lambda x: x['text'], validate=False)\n",
    "\n",
    "# Obtain the numeric data: get_numeric_data\n",
    "get_numeric_data = FunctionTransformer(lambda x: x[['numeric', 'with_missing']], validate=False)\n",
    "\n",
    "# Fit and transform the text data: just_text_data\n",
    "just_text_data = get_text_data.fit_transform(sample_df)\n",
    "\n",
    "# Fit and transform the numeric data: just_numeric_data\n",
    "just_numeric_data = get_numeric_data.fit_transform(sample_df)\n",
    "\n",
    "# Print head to check results\n",
    "print('Text Data')\n",
    "print(just_text_data.head())\n",
    "print('\\nNumeric Data')\n",
    "print(just_numeric_data.head())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature union, multiple processing types\n",
    "\n",
    "Using FeatureUnion(), you can perform separate steps on text and numeric data in your pipeline.\n",
    "\n",
    "With these tools, all preprocessing steps for your model can be simplified, even for models with multiple data types. For example, you don't want to impute our text data, nor do you want to create a bag-of-words with our numeric data. Instead, you want to deal with these separately and then join the results together using FeatureUnion().\n",
    "\n",
    "In the end, you'll still have only two high-level steps in your pipeline: preprocessing and model instantiation. The difference is that the first preprocessing step actually consists of a pipeline for numeric data and a pipeline for text data. The results of those pipelines are joined using FeatureUnion()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on sample data - all data:  0.928\n"
     ]
    }
   ],
   "source": [
    "# Import FeatureUnion\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# Split using ALL data in sample_df\n",
    "X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric', 'with_missing', 'text']],\n",
    "                                                    pd.get_dummies(sample_df['label']),\n",
    "                                                    random_state=22)\n",
    "\n",
    "# Create a FeatureUnion with nested pipeline: process_and_join_features\n",
    "process_and_join_features = FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', imputer)\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', CountVectorizer())\n",
    "                ]))\n",
    "             ]\n",
    "        )\n",
    "\n",
    "# Instantiate nested pipeline: pl\n",
    "pl = Pipeline([\n",
    "        ('union', process_and_join_features),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])\n",
    "\n",
    "\n",
    "# Fit pl to the training data\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on sample data - all data: \", accuracy)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Choosing a classification model\n",
    "### Using FunctionTransformer on the main dataset\n",
    "\n",
    "In this exercise you're going to use FunctionTransformer on the primary budget data, before instantiating a multiple-datatype pipeline in the next exercise."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "dummy_labels = pd.get_dummies(df[LABELS])\n",
    "\n",
    "# Get the columns that are features in the original df\n",
    "NON_LABELS = [c for c in df.columns if c not in LABELS]\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = multilabel_train_test_split(df[NON_LABELS],\n",
    "                                                               dummy_labels,\n",
    "                                                              size=0.2,\n",
    "                                                              seed=123)\n",
    "\n",
    "# Preprocess the text data: get_text_data\n",
    "get_text_data = FunctionTransformer(combine_text_columns, validate=False)\n",
    "\n",
    "# Preprocess the numeric data: get_numeric_data\n",
    "get_numeric_data = FunctionTransformer(lambda x: x[NUMERIC_COLUMNS], validate=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "outputs": [
    {
     "data": {
      "text/plain": "                    Function          Use          Sharing   Reporting  \\\n198                 NO_LABEL     NO_LABEL         NO_LABEL    NO_LABEL   \n209   Student Transportation     NO_LABEL  Shared Services  Non-School   \n750     Teacher Compensation  Instruction  School Reported      School   \n931                 NO_LABEL     NO_LABEL         NO_LABEL    NO_LABEL   \n1524                NO_LABEL     NO_LABEL         NO_LABEL    NO_LABEL   \n\n     Student_Type Position_Type               Object_Type     Pre_K  \\\n198      NO_LABEL      NO_LABEL                  NO_LABEL  NO_LABEL   \n209      NO_LABEL      NO_LABEL    Other Non-Compensation  NO_LABEL   \n750   Unspecified       Teacher  Base Salary/Compensation  Non PreK   \n931      NO_LABEL      NO_LABEL                  NO_LABEL  NO_LABEL   \n1524     NO_LABEL      NO_LABEL                  NO_LABEL  NO_LABEL   \n\n       Operating_Status               Object_Description  ...  \\\n198       Non-Operating                   Supplemental *  ...   \n209   PreK-12 Operating  REPAIR AND MAINTENANCE SERVICES  ...   \n750   PreK-12 Operating     Personal Services - Teachers  ...   \n931       Non-Operating                 General Supplies  ...   \n1524      Non-Operating           Supplies and Materials  ...   \n\n                   Sub_Object_Description Location_Description  FTE  \\\n198   Non-Certificated Salaries And Wages                  NaN  NaN   \n209                                   NaN      ADMIN. SERVICES  NaN   \n750                                   NaN                  NaN  1.0   \n931                      General Supplies                  NaN  NaN   \n1524               Supplies And Materials                  NaN  NaN   \n\n                      Function_Description      Facility_or_Department  \\\n198   Care and Upkeep of Building Services                         NaN   \n209              STUDENT TRANSPORT SERVICE                         NaN   \n750                                    NaN                         NaN   \n931                            Instruction  Instruction And Curriculum   \n1524            Other Community Services *                         NaN   \n\n     Position_Extra     Total  \\\n198                  -8291.86   \n209                    618.29   \n750         TEACHER  49768.82   \n931                     -1.02   \n1524                  2304.43   \n\n                                    Program_Description  \\\n198                                                 NaN   \n209                                PUPIL TRANSPORTATION   \n750                               Instruction - Regular   \n931   \"Title I, Part A Schoolwide Activities Related...   \n1524                                                NaN   \n\n                                       Fund_Description                Text_1  \n198   Title I - Disadvantaged Children/Targeted Assi...    TITLE I CARRYOVER   \n209                                        General Fund                   NaN  \n750                              General Purpose School                   NaN  \n931                              General Operating Fund                   NaN  \n1524  Title I - Disadvantaged Children/Targeted Assi...   TITLE I PI+HOMELESS  \n\n[5 rows x 25 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Function</th>\n      <th>Use</th>\n      <th>Sharing</th>\n      <th>Reporting</th>\n      <th>Student_Type</th>\n      <th>Position_Type</th>\n      <th>Object_Type</th>\n      <th>Pre_K</th>\n      <th>Operating_Status</th>\n      <th>Object_Description</th>\n      <th>...</th>\n      <th>Sub_Object_Description</th>\n      <th>Location_Description</th>\n      <th>FTE</th>\n      <th>Function_Description</th>\n      <th>Facility_or_Department</th>\n      <th>Position_Extra</th>\n      <th>Total</th>\n      <th>Program_Description</th>\n      <th>Fund_Description</th>\n      <th>Text_1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>198</th>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>Non-Operating</td>\n      <td>Supplemental *</td>\n      <td>...</td>\n      <td>Non-Certificated Salaries And Wages</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Care and Upkeep of Building Services</td>\n      <td>NaN</td>\n      <td></td>\n      <td>-8291.86</td>\n      <td>NaN</td>\n      <td>Title I - Disadvantaged Children/Targeted Assi...</td>\n      <td>TITLE I CARRYOVER</td>\n    </tr>\n    <tr>\n      <th>209</th>\n      <td>Student Transportation</td>\n      <td>NO_LABEL</td>\n      <td>Shared Services</td>\n      <td>Non-School</td>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>Other Non-Compensation</td>\n      <td>NO_LABEL</td>\n      <td>PreK-12 Operating</td>\n      <td>REPAIR AND MAINTENANCE SERVICES</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>ADMIN. SERVICES</td>\n      <td>NaN</td>\n      <td>STUDENT TRANSPORT SERVICE</td>\n      <td>NaN</td>\n      <td></td>\n      <td>618.29</td>\n      <td>PUPIL TRANSPORTATION</td>\n      <td>General Fund</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>750</th>\n      <td>Teacher Compensation</td>\n      <td>Instruction</td>\n      <td>School Reported</td>\n      <td>School</td>\n      <td>Unspecified</td>\n      <td>Teacher</td>\n      <td>Base Salary/Compensation</td>\n      <td>Non PreK</td>\n      <td>PreK-12 Operating</td>\n      <td>Personal Services - Teachers</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>TEACHER</td>\n      <td>49768.82</td>\n      <td>Instruction - Regular</td>\n      <td>General Purpose School</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>931</th>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>Non-Operating</td>\n      <td>General Supplies</td>\n      <td>...</td>\n      <td>General Supplies</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Instruction</td>\n      <td>Instruction And Curriculum</td>\n      <td></td>\n      <td>-1.02</td>\n      <td>\"Title I, Part A Schoolwide Activities Related...</td>\n      <td>General Operating Fund</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1524</th>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>NO_LABEL</td>\n      <td>Non-Operating</td>\n      <td>Supplies and Materials</td>\n      <td>...</td>\n      <td>Supplies And Materials</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Other Community Services *</td>\n      <td>NaN</td>\n      <td></td>\n      <td>2304.43</td>\n      <td>NaN</td>\n      <td>Title I - Disadvantaged Children/Targeted Assi...</td>\n      <td>TITLE I PI+HOMELESS</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã 25 columns</p>\n</div>"
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Add a model to the pipeline\n",
    "\n",
    "Youâre about to take everything youâve learned so far and implement it in a Pipeline that works with the real, DrivenData budget line item data youâve been exploring.\n",
    "\n",
    "Surprise! The structure of the pipeline is exactly the same as earlier in this chapter:\n",
    "\n",
    "    * the preprocessing step uses FeatureUnion to join the results of nested pipelines that each rely on FunctionTransformer to select multiple datatypes\n",
    "    * the model step stores the model object"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on budget dataset:  0.928\n"
     ]
    }
   ],
   "source": [
    "pl = Pipeline([\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector', get_numeric_data),\n",
    "                ('imputer', imputer)\n",
    "            ])),\n",
    "            ('text_features', Pipeline([\n",
    "                ('selector', get_text_data),\n",
    "                ('vectorizer', CountVectorizer())\n",
    "            ]))\n",
    "        ]\n",
    "    )),\n",
    "    ('clf', OneVsRestClassifier(LogisticRegression(max_iter=1000), n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Fit to the training data\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on budget dataset: \", accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Try a different class of model\n",
    "\n",
    "Now you're cruising. One of the great strengths of pipelines is how easy they make the process of testing different models.\n",
    "\n",
    "Until now, you've been using the model step ('clf', OneVsRestClassifier(LogisticRegression())) in your pipeline.\n",
    "\n",
    "But what if you want to try a different model? Do you need to build an entirely new pipeline? New nests? New FeatureUnions? Nope! You just have a simple one-line change, as you'll see in this exercise.\n",
    "\n",
    "In particular, you'll swap out the logistic-regression model and replace it with a random forest classifier, which uses the statistics of an ensemble of decision trees to generate predictions."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on budget dataset:  0.924\n"
     ]
    }
   ],
   "source": [
    "# Import random forest classifer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Edit model step in pipeline\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', imputer)\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', CountVectorizer())\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('clf', RandomForestClassifier())\n",
    "    ])\n",
    "\n",
    "# Fit to the training data\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on budget dataset: \", accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Can you adjust the model or parameters to improve accuracy?\n",
    "\n",
    "You just saw a substantial improvement in accuracy by swapping out the model. Pipelines are amazing!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on budget dataset:  0.916\n"
     ]
    }
   ],
   "source": [
    "# Import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Add model step to pipeline: pl\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', imputer)\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', CountVectorizer())\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('clf', RandomForestClassifier(n_estimators=15))\n",
    "    ])\n",
    "\n",
    "# Fit to the training data\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on budget dataset: \", accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Learning from the experts\n",
    "\n",
    "Text preprocessing\n",
    "\n",
    "    * NLP tricks for text data\n",
    "\n",
    "        * Tokenize on punctuation to avoid hyphens, underscores, etc.\n",
    "\n",
    "        * Includes unigrams and bi-grams in the model to capture important information involving multiple tokens - e.g. \"middle school\"\n",
    "\n",
    "### Deciding whatâs a word\n",
    "\n",
    "Before you build up to the winning pipeline, it will be useful to look a little deeper into how the text features will be processed.\n",
    "\n",
    "In this exercise, you will useÂ CountVectorizerÂ on the training dataÂ X_trainÂ (preloaded into the workspace) to see the effect of tokenization on punctuation.\n",
    "\n",
    "Remember, sinceÂ CountVectorizerÂ expects a vector, youâll need to use the preloaded function,Â combine_text_columnsÂ before fitting to the training data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, float found",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [421]\u001B[0m, in \u001B[0;36m<cell line: 5>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeature_extraction\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtext\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CountVectorizer\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# Create the text vector\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m text_vector \u001B[38;5;241m=\u001B[39m \u001B[43mcombine_text_columns\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# Create the token pattern: TOKENS_ALPHANUMERIC\u001B[39;00m\n\u001B[0;32m      8\u001B[0m TOKENS_ALPHANUMERIC \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m[A-Za-z0-9]+(?=\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124ms+)\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
      "Input \u001B[1;32mIn [408]\u001B[0m, in \u001B[0;36mcombine_text_columns\u001B[1;34m(data_frame, to_drop)\u001B[0m\n\u001B[0;32m     10\u001B[0m text_data\u001B[38;5;241m.\u001B[39mfillna(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m,inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# Join all text items in a row that have a space in between\u001B[39;00m\n\u001B[1;32m---> 13\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtext_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m \u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py:8839\u001B[0m, in \u001B[0;36mDataFrame.apply\u001B[1;34m(self, func, axis, raw, result_type, args, **kwargs)\u001B[0m\n\u001B[0;32m   8828\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapply\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m frame_apply\n\u001B[0;32m   8830\u001B[0m op \u001B[38;5;241m=\u001B[39m frame_apply(\n\u001B[0;32m   8831\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   8832\u001B[0m     func\u001B[38;5;241m=\u001B[39mfunc,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   8837\u001B[0m     kwargs\u001B[38;5;241m=\u001B[39mkwargs,\n\u001B[0;32m   8838\u001B[0m )\n\u001B[1;32m-> 8839\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39m__finalize__(\u001B[38;5;28mself\u001B[39m, method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapply\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\apply.py:727\u001B[0m, in \u001B[0;36mFrameApply.apply\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    724\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw:\n\u001B[0;32m    725\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_raw()\n\u001B[1;32m--> 727\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\apply.py:851\u001B[0m, in \u001B[0;36mFrameApply.apply_standard\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    850\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply_standard\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m--> 851\u001B[0m     results, res_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_series_generator\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    853\u001B[0m     \u001B[38;5;66;03m# wrap results\u001B[39;00m\n\u001B[0;32m    854\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwrap_results(results, res_index)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\apply.py:867\u001B[0m, in \u001B[0;36mFrameApply.apply_series_generator\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    864\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m option_context(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmode.chained_assignment\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    865\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, v \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(series_gen):\n\u001B[0;32m    866\u001B[0m         \u001B[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001B[39;00m\n\u001B[1;32m--> 867\u001B[0m         results[i] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43mv\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    868\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(results[i], ABCSeries):\n\u001B[0;32m    869\u001B[0m             \u001B[38;5;66;03m# If we have a view on v, we need to make a copy because\u001B[39;00m\n\u001B[0;32m    870\u001B[0m             \u001B[38;5;66;03m#  series_generator will swap out the underlying data\u001B[39;00m\n\u001B[0;32m    871\u001B[0m             results[i] \u001B[38;5;241m=\u001B[39m results[i]\u001B[38;5;241m.\u001B[39mcopy(deep\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "Input \u001B[1;32mIn [408]\u001B[0m, in \u001B[0;36mcombine_text_columns.<locals>.<lambda>\u001B[1;34m(x)\u001B[0m\n\u001B[0;32m     10\u001B[0m text_data\u001B[38;5;241m.\u001B[39mfillna(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m,inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# Join all text items in a row that have a space in between\u001B[39;00m\n\u001B[1;32m---> 13\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m text_data\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m \u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[1;31mTypeError\u001B[0m: sequence item 0: expected str instance, float found"
     ]
    }
   ],
   "source": [
    "# Import the CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create the text vector\n",
    "text_vector = combine_text_columns(X_train)\n",
    "\n",
    "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "\n",
    "# Instantiate the CountVectorizer: text_features\n",
    "text_features = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n",
    "\n",
    "# Fit text_features to the text vector\n",
    "text_features.fit(text_vector)\n",
    "\n",
    "# Print the first 10 tokens\n",
    "print(text_features.get_feature_names()[:10])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### N-gram range in scikit-learn\n",
    "\n",
    "In this exercise you'll insert a CountVectorizer instance into your pipeline for the main dataset, and compute multiple n-gram features to be used in the model.\n",
    "\n",
    "In order to look for ngram relationships at multiple scales, you will use the ngram_range parameter as Peter discussed in the video.\n",
    "\n",
    "Special functions: You'll notice a couple of new steps provided in the pipeline in this and many of the remaining exercises. Specifically, the dim_red step following the vectorizer step , and the scale step preceeding the clf (classification) step.\n",
    "\n",
    "These have been added in order to account for the fact that you're using a reduced-size sample of the full dataset in this course. To make sure the models perform as the expert competition winner intended, we have to apply a dimensionality reduction technique, which is what the dim_red step does, and we have to scale the features to lie between -1 and 1, which is what the scale step does.\n",
    "\n",
    "The dim_red step uses a scikit-learn function called SelectKBest(), applying something called the chi-squared test to select the K \"best\" features. The scale step uses a scikit-learn function called MaxAbsScaler() in order to squash the relevant features into the interval -1 to 1.\n",
    "\n",
    "You won't need to do anything extra with these functions here, just complete the vectorizing pipeline steps below. However, notice how easy it was to add more processing steps to our pipeline!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "# Select 300 best features\n",
    "chi_k = 300\n",
    "\n",
    "# Perform preprocessing\n",
    "get_text_data = FunctionTransformer(combine_text_columns, validate=False)\n",
    "get_numeric_data = FunctionTransformer(lambda x: x[NUMERIC_COLUMNS], validate=False)\n",
    "\n",
    "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "\n",
    "# Instantiate pipeline: pl\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', SimpleImputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
    "                                                   ngram_range=(1, 2))),\n",
    "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('scale', MaxAbsScaler()),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression(max_iter=1000)))\n",
    "    ])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on budget dataset: \", accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Learning from the expert: a stats trick\n",
    "\n",
    "* Interaction terms\n",
    "\n",
    "    * Example\n",
    "      * English teacher for 2nd grade\n",
    "      * 2nd grade - budget for English teacher\n",
    "    * Interaction terms mathematically describe when tokens appear together\n",
    "    * the math:\n",
    "\n",
    "\n",
    "Î²1âx1â+Î²2âx2â+Î²3â(x1âÃx2â)\n",
    "### Implement interaction modeling in scikit-learn\n",
    "\n",
    "It's time to add interaction features to your model. The PolynomialFeatures object in scikit-learn does just that, but here you're going to use a custom interaction object, SparseInteractions. Interaction terms are a statistical tool that lets your model express what happens if two features appear together in the same row.\n",
    "\n",
    "SparseInteractions does the same thing as PolynomialFeatures, but it uses sparse matrices to do so. You can get the code for SparseInteractions at this GitHub Gist.\n",
    "\n",
    "PolynomialFeatures and SparseInteractions both take the argument degree, which tells them what polynomial degree of interactions to compute.\n",
    "\n",
    "You're going to consider interaction terms of degree=2 in your pipeline. You will insert these steps after the preprocessing steps you've built out so far, but before the classifier steps.\n",
    "\n",
    "Pipelines with interaction terms take a while to train (since you're making n features into n-squared features!), so as long as you set it up right, we'll do the heavy lifting and tell you what your score is!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class SparseInteractions(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, degree=2, feature_name_separator=\"_\"):\n",
    "        self.degree = degree\n",
    "        self.feature_name_separator = feature_name_separator\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if not sparse.isspmatrix_csc(X):\n",
    "            X = sparse.csc_matrix(X)\n",
    "\n",
    "        if hasattr(X, \"columns\"):\n",
    "            self.orig_col_names = X.columns\n",
    "        else:\n",
    "            self.orig_col_names = np.array([str(i) for i in range(X.shape[1])])\n",
    "\n",
    "        spi = self._create_sparse_interactions(X)\n",
    "        return spi\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return self.feature_names\n",
    "\n",
    "    def _create_sparse_interactions(self, X):\n",
    "        out_mat = []\n",
    "        self.feature_names = self.orig_col_names.tolist()\n",
    "\n",
    "        for sub_degree in range(2, self.degree + 1):\n",
    "            for col_ixs in combinations(range(X.shape[1]), sub_degree):\n",
    "                # add name for new column\n",
    "                name = self.feature_name_separator.join(self.orig_col_names[list(col_ixs)])\n",
    "                self.feature_names.append(name)\n",
    "\n",
    "                # get column multiplications value\n",
    "                out = X[:, col_ixs[0]]\n",
    "                for j in col_ixs[1:]:\n",
    "                    out = out.multiply(X[:, j])\n",
    "\n",
    "                out_mat.append(out)\n",
    "\n",
    "        return sparse.hstack([X] + out_mat)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', SimpleImputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
    "                                                   ngram_range=(1, 2))),\n",
    "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('int', SparseInteractions(degree=2)),\n",
    "        ('scale', MaxAbsScaler()),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression(max_iter=1000)))\n",
    "    ])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on sparse interaction: \", accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Learning from the expert the winning model\n",
    "\n",
    "    * The hashing trick\n",
    "        * Adding new features may cause enormous increase in array size\n",
    "        * Hashing is a way of increasing memory efficiency\n",
    "            * Hash function limits possible outputs, fixing array size\n",
    "    * When to use the hashing trick\n",
    "        * Want to make array of features as small as possible\n",
    "            * Dimensionality reduction\n",
    "            * Particularly useful on large datasets\n",
    "            * E.g., lots of text data!\n",
    "\n",
    "### Why is hashing a useful trick?\n",
    "\n",
    "In the video, Peter explained that a hash function takes an input, in your case a token, and outputs a hash value. For example, the input may be a string and the hash value may be an integer.\n",
    "\n",
    "By explicitly stating how many possible outputs the hashing function may have, we limit the size of the objects that need to be processed. With these limits known, computation can be made more efficient and we can get results faster, even on large datasets.\n",
    "\n",
    "### Implementing the hashing trick in scikit-learn\n",
    "\n",
    "In this exercise you will check out the scikit-learn implementation of HashingVectorizer before adding it to your pipeline later.\n",
    "\n",
    "As you saw in the video, HashingVectorizer acts just like CountVectorizer in that it can accept token_pattern and ngram_range parameters. The important difference is that it creates hash values from the text, so that we get all the computational advantages of hashing!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, float found",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [422]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeature_extraction\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtext\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m HashingVectorizer\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Get text data: text_data\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m text_data \u001B[38;5;241m=\u001B[39m \u001B[43mcombine_text_columns\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# Create the token pattern: TOKENS_ALPHANUMERIC\u001B[39;00m\n\u001B[0;32m      7\u001B[0m TOKENS_ALPHANUMERIC \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m[A-Za-z0-9]+(?=\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124ms+)\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
      "Input \u001B[1;32mIn [408]\u001B[0m, in \u001B[0;36mcombine_text_columns\u001B[1;34m(data_frame, to_drop)\u001B[0m\n\u001B[0;32m     10\u001B[0m text_data\u001B[38;5;241m.\u001B[39mfillna(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m,inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# Join all text items in a row that have a space in between\u001B[39;00m\n\u001B[1;32m---> 13\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtext_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m \u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py:8839\u001B[0m, in \u001B[0;36mDataFrame.apply\u001B[1;34m(self, func, axis, raw, result_type, args, **kwargs)\u001B[0m\n\u001B[0;32m   8828\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapply\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m frame_apply\n\u001B[0;32m   8830\u001B[0m op \u001B[38;5;241m=\u001B[39m frame_apply(\n\u001B[0;32m   8831\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   8832\u001B[0m     func\u001B[38;5;241m=\u001B[39mfunc,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   8837\u001B[0m     kwargs\u001B[38;5;241m=\u001B[39mkwargs,\n\u001B[0;32m   8838\u001B[0m )\n\u001B[1;32m-> 8839\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39m__finalize__(\u001B[38;5;28mself\u001B[39m, method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapply\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\apply.py:727\u001B[0m, in \u001B[0;36mFrameApply.apply\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    724\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw:\n\u001B[0;32m    725\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_raw()\n\u001B[1;32m--> 727\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\apply.py:851\u001B[0m, in \u001B[0;36mFrameApply.apply_standard\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    850\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply_standard\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m--> 851\u001B[0m     results, res_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_series_generator\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    853\u001B[0m     \u001B[38;5;66;03m# wrap results\u001B[39;00m\n\u001B[0;32m    854\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwrap_results(results, res_index)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\apply.py:867\u001B[0m, in \u001B[0;36mFrameApply.apply_series_generator\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    864\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m option_context(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmode.chained_assignment\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    865\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, v \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(series_gen):\n\u001B[0;32m    866\u001B[0m         \u001B[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001B[39;00m\n\u001B[1;32m--> 867\u001B[0m         results[i] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43mv\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    868\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(results[i], ABCSeries):\n\u001B[0;32m    869\u001B[0m             \u001B[38;5;66;03m# If we have a view on v, we need to make a copy because\u001B[39;00m\n\u001B[0;32m    870\u001B[0m             \u001B[38;5;66;03m#  series_generator will swap out the underlying data\u001B[39;00m\n\u001B[0;32m    871\u001B[0m             results[i] \u001B[38;5;241m=\u001B[39m results[i]\u001B[38;5;241m.\u001B[39mcopy(deep\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "Input \u001B[1;32mIn [408]\u001B[0m, in \u001B[0;36mcombine_text_columns.<locals>.<lambda>\u001B[1;34m(x)\u001B[0m\n\u001B[0;32m     10\u001B[0m text_data\u001B[38;5;241m.\u001B[39mfillna(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m,inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# Join all text items in a row that have a space in between\u001B[39;00m\n\u001B[1;32m---> 13\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m text_data\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m \u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[1;31mTypeError\u001B[0m: sequence item 0: expected str instance, float found"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# Get text data: text_data\n",
    "text_data = combine_text_columns(X_train)\n",
    "\n",
    "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "\n",
    "# Instantiate the HashingVectorizer: hashing_vec\n",
    "hashing_vec = HashingVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n",
    "\n",
    "# Fit and transform the Hashing Vectorizer\n",
    "hashed_text = hashing_vec.fit_transform(text_data)\n",
    "\n",
    "# Create DataFrame and print the head\n",
    "hashed_df = pd.DataFrame(hashed_text.data)\n",
    "print(hashed_df.head())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Build the winning model\n",
    "\n",
    "You have arrived! This is where all of your hard work pays off. It's time to build the model that won DrivenData's competition.\n",
    "\n",
    "You've constructed a robust, powerful pipeline capable of processing training and testing data. Now that you understand the data and know all of the tools you need, you can essentially solve the whole problem in a relatively small number of lines of code. Wow!\n",
    "\n",
    "All you need to do is add the HashingVectorizer step to the pipeline to replace the CountVectorizer step.\n",
    "\n",
    "The parameters non_negative=True, norm=None, and binary=False make the HashingVectorizer perform similarly to the default settings on the CountVectorizer so you can just replace one with the other."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes from 1 to 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [423]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m pl \u001B[38;5;241m=\u001B[39m Pipeline([\n\u001B[0;32m      2\u001B[0m         (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124munion\u001B[39m\u001B[38;5;124m'\u001B[39m, FeatureUnion(\n\u001B[0;32m      3\u001B[0m             transformer_list \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m      4\u001B[0m                 (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnumeric_features\u001B[39m\u001B[38;5;124m'\u001B[39m, Pipeline([\n\u001B[0;32m      5\u001B[0m                     (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mselector\u001B[39m\u001B[38;5;124m'\u001B[39m, get_numeric_data),\n\u001B[0;32m      6\u001B[0m                     (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimputer\u001B[39m\u001B[38;5;124m'\u001B[39m, SimpleImputer())\n\u001B[0;32m      7\u001B[0m                 ])),\n\u001B[0;32m      8\u001B[0m                 (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext_features\u001B[39m\u001B[38;5;124m'\u001B[39m, Pipeline([\n\u001B[0;32m      9\u001B[0m                     (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mselector\u001B[39m\u001B[38;5;124m'\u001B[39m, get_text_data),\n\u001B[0;32m     10\u001B[0m                     (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvectorizer\u001B[39m\u001B[38;5;124m'\u001B[39m, HashingVectorizer(token_pattern\u001B[38;5;241m=\u001B[39mTOKENS_ALPHANUMERIC,\n\u001B[0;32m     11\u001B[0m                                                      norm\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, binary\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m     12\u001B[0m                                                      ngram_range\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m))),\n\u001B[1;32m---> 13\u001B[0m                     (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdim_red\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[43mSelectKBest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchi2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchi_k\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     14\u001B[0m                 ]))\n\u001B[0;32m     15\u001B[0m              ]\n\u001B[0;32m     16\u001B[0m         )),\n\u001B[0;32m     17\u001B[0m         (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mint\u001B[39m\u001B[38;5;124m'\u001B[39m, SparseInteractions(degree\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)),\n\u001B[0;32m     18\u001B[0m         (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mscale\u001B[39m\u001B[38;5;124m'\u001B[39m, MaxAbsScaler()),\n\u001B[0;32m     19\u001B[0m         (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mclf\u001B[39m\u001B[38;5;124m'\u001B[39m, OneVsRestClassifier(LogisticRegression()))\n\u001B[0;32m     20\u001B[0m     ])\n",
      "\u001B[1;31mTypeError\u001B[0m: __init__() takes from 1 to 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', SimpleImputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', HashingVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
    "                                                     norm=None, binary=False,\n",
    "                                                     ngram_range=(1, 2))),\n",
    "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('int', SparseInteractions(degree=2)),\n",
    "        ('scale', MaxAbsScaler()),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}